# Fine-Tuning Llama Models with Unsloth

This repository contains a complete workflow for fine-tuning Llama-based Large Language Models (LLMs) using the Unsloth library. The notebook demonstrates how to prepare data, define prompt templates, train efficiently, and export the final model for inference.

Unsloth is an optimized training framework that enables fast, memory-efficient, and cost-effective fine-tuning of LLMs on consumer GPUs.
This project walks through:

Loading and configuring a Llama model

Preparing custom training data

Applying Prompt Templates (ChatML format)

Performing LoRA-based fine-tuning

Evaluating and saving the model

Exporting for inference (HuggingFace / GGUF)
